import requests
from bs4 import BeautifulSoup
import os
import json
import jieba
import pandas as pd
import glob


userAgent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36 Edg/86.0.622.51'
headers = {'User-Agent':userAgent}

n = input('關鍵字:')
url = 'https://www.104.com.tw/jobs/search/?keyword=' + str(n)
if not os.path.exists('./%s'%n) :
    os.mkdir('./%s' %n)
# url = 'https://www.104.com.tw/jobs/search/?keyword=' + '資料工程師'
# print(url)

skill_list = ['python','r','java','javascript','mysql','mongodb','nosql','sql','aws','gcp',
              'azure','data-mining','linux','docker','deep-learning','cloud-service','spark','hadoop','ai','iot']

h= 0
for i in range(0,10) :
    res = requests.get(url, headers=headers)
    soup = BeautifulSoup(res.text, 'html.parser')
    titleSoupList = soup.select('article[class="b-block--top-bord job-list-item b-clearfix js-job-item"]')
    num =1
    for titleSoup in titleSoupList:

        #職稱
        titleList = titleSoup.text
        title =titleList.replace(' ', '').split('\n')[6]
        # print(title)

        #工作內容
        jobsurlList = titleSoup.select('h2.b-tit a')
        # print(jobsurlList)
        for jobsurlsoup in jobsurlList :

            jobsurl = 'https://www.104.com.tw/job/ajax/content/' + jobsurlsoup['href'].split('/')[-1]
            referer = jobsurl.split('?')[0]
            headers1 = {
                'User-Agent': userAgent ,
                'Referer' : referer
            }
            jobsres = requests.get(url = jobsurl, headers=headers1)
            jobscontent = json.loads(jobsres.text)['data']['jobDetail']['jobDescription'].replace(' ','-')
        # print(jobscontent)

        #公司
        companySoupList = titleSoup.select('ul[class="b-list-inline b-clearfix"]')
        for companySoup in companySoupList:
            company = companySoup.select('a')[0].text.replace(' ','').replace('\n','')
        # print(company)

        #地區
        areaSoupList = titleSoup.select('ul[class="b-list-inline b-clearfix job-list-intro b-content"]')
        for areaSoup in areaSoupList :
            area = areaSoup.text.split('\n')[1]
        # print(area)

        #經歷
        for experienceSoup in areaSoupList :
            experience = experienceSoup.text.split('\n')[3]
        # print(experience)

        #學歷
        for educationSoup in areaSoupList :
            education = educationSoup.text.split('\n')[5]
        # print(education)

        #薪水
        salarySoupList = titleSoup.select('div[class="job-list-tag b-content"]')
        for salarySoup in salarySoupList :
            salary = salarySoup.text.split('\n')[1]
        # print(salary)

        #連結
        articleSoupList = titleSoup.select('a[class="js-job-link"]')
        for articleSoup in articleSoupList:
            articleurl = 'https:' + articleSoup['href']
        # print(articleurl)

        allcontent = '職稱: %s\n' % (title)
        allcontent += '公司: %s\n' % (company)
        allcontent += '地區: %s\n' % (area)
        allcontent += '經歷: %s\n' % (experience)
        allcontent += '學歷: %s\n' % (education)
        allcontent += '薪資: %s\n' % (salary)
        allcontent += '連結: %s\n' % (articleurl)
        allcontent += '\n'
        # company += '(%s)' %(num)
        company += '%s' %title
        print(company)
        print(articleurl)

        wordlist = jieba.cut(str(jobscontent).lower())
        jieba.load_userdict('./mydict.txt')
        cutlist = [l for l in wordlist]
        for m in skill_list:
            if m in cutlist:
                allcontent += m + ': 1\n'
            else:
                allcontent += m + ': 0\n'

        num += 1

        try:
            with open("./%s/%s.txt" % (n, company), 'w', encoding='utf-8') as f:
                f.write(allcontent)

        except :
            pass
        # except FileNotFoundError as e:
        #     filename = filename.replace('/', '-')
        #     with open(f"./{n}/{filename}.txt", 'w', encoding='utf-8') as f:
        #         f.write(allcontent)
        # except OSError:
        #     pass


    print('='*20)
    h += 1
    print('=====%s======' %h)


    #Find new url
    newUrl = 'https://www.104.com.tw/jobs/search/?keyword=' + str(n) + '&page=' + str(h)
    # newUrl = 'https://www.104.com.tw/jobs/search/?keyword=' + '資料工程師' + '&page=' + str(h)
    # print(newUrl)
    #print(newurl)
    url = newUrl

glob.glob('./資料工程師/*.txt')

rowData = list()
for article in glob.glob('./資料工程師/*.txt'):
    with open(article, 'r', encoding='utf-8') as f:
        tmplist = f.read().split('\n')[0:-1]
        rowData.append(tmplist)

columns = [r.split(':')[0] for r in rowData[0]]
df = pd.DataFrame(data=rowData, columns=columns)

def column_filter(s):
    return s.split(':')[-1]
df3 = df.copy()
for c in df3:
    df3[c] = df3[c].apply(lambda x: x.split(': ')[-1])

df3.to_csv('./104銀行%s.csv' %n, index=False, encoding='utf-8-sig')
